"""
ë¬¸ì„œ ì²˜ë¦¬ ì„œë¹„ìŠ¤
"""
import logging
import os
from typing import Optional, List
from sqlalchemy.orm import Session

from app.models.document import Document
from app.models.embedding import Embedding
from app.services.openai_service import get_openai_service
from app.config import settings

logger = logging.getLogger(__name__)

class DocumentProcessingService:
    """ë¬¸ì„œ ì²˜ë¦¬ ì„œë¹„ìŠ¤"""
    
    def __init__(self):
        self.openai_service = get_openai_service()
    
    async def process_document(self, document: Document, db: Session) -> bool:
        """
        ë¬¸ì„œ ì²˜ë¦¬ (ë¶„ì„ + ì„ë² ë”© ìƒì„±)
        
        Args:
            document: ì²˜ë¦¬í•  ë¬¸ì„œ
            db: ë°ì´í„°ë² ì´ìŠ¤ ì„¸ì…˜
            
        Returns:
            ì²˜ë¦¬ ì„±ê³µ ì—¬ë¶€
        """
        try:
            logger.info(f"ë¬¸ì„œ ì²˜ë¦¬ ì‹œì‘: {document.id} - {document.filename}")
            
            # ë¬¸ì„œ ìƒíƒœë¥¼ PROCESSINGìœ¼ë¡œ ë³€ê²½
            document.status = "PROCESSING"
            db.commit()
            
            # ë¬¸ì„œ ë‚´ìš© ì½ê¸°
            content = await self._read_document_content(document)
            if not content:
                logger.error(f"ë¬¸ì„œ ë‚´ìš©ì„ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {document.id}")
                document.status = "FAILED"
                db.commit()
                return False
            
            # ë¬¸ì„œ ë¶„ì„
            analysis_result = await self.openai_service.analyze_document(content)
            
            # ë¬¸ì„œ ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸
            document.summary = analysis_result.get("summary", "")
            document.keywords = analysis_result.get("keywords", [])
            document.category = analysis_result.get("category", "ì¼ë°˜")
            document.topic = analysis_result.get("topic", "ê¸°íƒ€")
            document.difficulty = analysis_result.get("difficulty", "ì¤‘ê¸‰")
            
            # í…ìŠ¤íŠ¸ ì²­í‚¹ ë° ì„ë² ë”© ìƒì„±
            logger.info(f"ğŸ“ í…ìŠ¤íŠ¸ ì²­í‚¹ ì‹œì‘: document_id={document.id}")
            chunks = self._split_text_into_chunks(content)
            logger.info(f"ğŸ“Š ì²­í‚¹ ì™„ë£Œ: {len(chunks)}ê°œ ì²­í¬ ìƒì„±")
            
            logger.info(f"ğŸ¯ ì„ë² ë”© ìƒì„± ì‹œì‘: document_id={document.id}")
            embeddings_created = await self._create_embeddings(document, chunks, db)
            logger.info(f"ğŸ¯ ì„ë² ë”© ìƒì„± ì™„ë£Œ: success={embeddings_created}")
            
            if embeddings_created:
                document.status = "COMPLETED"
                logger.info(f"âœ… ë¬¸ì„œ ì²˜ë¦¬ ì™„ë£Œ: {document.id}")
            else:
                document.status = "FAILED"
                logger.error(f"âŒ ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {document.id}")
            
            logger.info(f"ğŸ’¾ ìµœì¢… ìƒíƒœ ì»¤ë°‹ ì‹œì‘: document_id={document.id}, status={document.status}")
            db.commit()
            logger.info(f"ğŸ’¾ ìµœì¢… ìƒíƒœ ì»¤ë°‹ ì™„ë£Œ: document_id={document.id}")
            
            return embeddings_created
            
        except Exception as e:
            logger.error(f"ë¬¸ì„œ ì²˜ë¦¬ ì‹¤íŒ¨: {document.id} - {str(e)}")
            document.status = "FAILED"
            db.commit()
            return False
    
    async def _read_document_content(self, document: Document) -> Optional[str]:
        """ë¬¸ì„œ ë‚´ìš© ì½ê¸°"""
        try:
            file_path = document.file_path
            if not os.path.exists(file_path):
                logger.error(f"íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {file_path}")
                return None
            
            # ì´ë¯¸ ì¶”ì¶œëœ ë‚´ìš©ì´ ìˆìœ¼ë©´ ë°˜í™˜
            if document.content:
                return document.content
            
            # íŒŒì¼ í™•ì¥ìì— ë”°ë¥¸ ë‚´ìš© ì¶”ì¶œ
            if document.file_type.lower() == "txt":
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                return content
            
            elif document.file_type.lower() == "pdf":
                # PDF ì²˜ë¦¬ (PyPDF2 ì‚¬ìš©)
                try:
                    import PyPDF2
                    with open(file_path, 'rb') as f:
                        pdf_reader = PyPDF2.PdfReader(f)
                        content = ""
                        for page in pdf_reader.pages:
                            content += page.extract_text() + "\n"
                    return content
                except Exception as e:
                    logger.error(f"PDF ì²˜ë¦¬ ì‹¤íŒ¨: {file_path} - {str(e)}")
                    return None
            
            else:
                logger.error(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹: {document.file_type}")
                return None
                
        except Exception as e:
            logger.error(f"ë¬¸ì„œ ë‚´ìš© ì½ê¸° ì‹¤íŒ¨: {document.file_path} - {str(e)}")
            return None
    
    def _split_text_into_chunks(self, text: str, chunk_size: int = 1000, overlap: int = 200) -> List[str]:
        """
        í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í• 
        
        Args:
            text: ë¶„í• í•  í…ìŠ¤íŠ¸
            chunk_size: ì²­í¬ í¬ê¸°
            overlap: ì¤‘ë³µ í¬ê¸°
            
        Returns:
            ë¶„í• ëœ í…ìŠ¤íŠ¸ ì²­í¬ ëª©ë¡
        """
        if not text:
            return []
        
        chunks = []
        start = 0
        
        while start < len(text):
            end = start + chunk_size
            
            # ë¬¸ì¥ ê²½ê³„ì—ì„œ ë¶„í• í•˜ë ¤ê³  ì‹œë„
            if end < len(text):
                # ë§ˆì§€ë§‰ ë¬¸ì¥ ë¶€í˜¸ë¥¼ ì°¾ê¸°
                last_period = text.rfind('.', start, end)
                last_exclamation = text.rfind('!', start, end)
                last_question = text.rfind('?', start, end)
                
                sentence_end = max(last_period, last_exclamation, last_question)
                if sentence_end > start:
                    end = sentence_end + 1
            
            chunk = text[start:end].strip()
            if chunk:
                chunks.append(chunk)
            
            # ë‹¤ìŒ ì‹œì‘ì  ì„¤ì • (ì¤‘ë³µ ê³ ë ¤)
            start = max(start + 1, end - overlap)
        
        logger.info(f"í…ìŠ¤íŠ¸ë¥¼ {len(chunks)}ê°œ ì²­í¬ë¡œ ë¶„í• ")
        return chunks
    
    async def _create_embeddings(self, document: Document, chunks: List[str], db: Session) -> bool:
        """
        ì²­í¬ë“¤ì— ëŒ€í•œ ì„ë² ë”© ìƒì„± ë° ì €ì¥
        
        Args:
            document: ë¬¸ì„œ ê°ì²´
            chunks: í…ìŠ¤íŠ¸ ì²­í¬ ëª©ë¡
            db: ë°ì´í„°ë² ì´ìŠ¤ ì„¸ì…˜
            
        Returns:
            ì„ë² ë”© ìƒì„± ì„±ê³µ ì—¬ë¶€
        """
        try:
            logger.info(f"ğŸ¯ ì„ë² ë”© ìƒì„± ì‹œì‘: document_id={document.id}, chunks={len(chunks)}")
            
            if not chunks:
                logger.warning(f"âš ï¸ ìƒì„±í•  ì²­í¬ê°€ ì—†ìŠµë‹ˆë‹¤: {document.id}")
                return True
            
            # ê¸°ì¡´ ì„ë² ë”© ì‚­ì œ (reprocessì—ì„œ ì´ë¯¸ í–ˆì§€ë§Œ ì•ˆì „ì„ ìœ„í•´)
            existing_count = db.query(Embedding).filter(Embedding.document_id == document.id).count()
            if existing_count > 0:
                logger.info(f"ğŸ—‘ï¸ ê¸°ì¡´ ì„ë² ë”© ì¶”ê°€ ì‚­ì œ: {existing_count}ê°œ")
                db.query(Embedding).filter(Embedding.document_id == document.id).delete()
            
            # ë°°ì¹˜ë¡œ ì„ë² ë”© ìƒì„±
            logger.info(f"ğŸš€ OpenAI ì„ë² ë”© ìƒì„± í˜¸ì¶œ: document_id={document.id}")
            embeddings = await self.openai_service.generate_embeddings_batch(chunks)
            logger.info(f"ğŸ“¦ OpenAI ì„ë² ë”© ì‘ë‹µ ë°›ìŒ: {len(embeddings)}ê°œ")
            
            if len(embeddings) != len(chunks):
                logger.error(f"âŒ ì„ë² ë”© ìˆ˜ì™€ ì²­í¬ ìˆ˜ê°€ ì¼ì¹˜í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {len(embeddings)} vs {len(chunks)}")
                return False
            
            # ì„ë² ë”© ì €ì¥
            logger.info(f"ğŸ’¾ ì„ë² ë”© ê°ì²´ ìƒì„± ë° DB ì €ì¥ ì‹œì‘: document_id={document.id}")
            for i, (chunk, embedding) in enumerate(zip(chunks, embeddings)):
                embedding_obj = Embedding(
                    document_id=document.id,
                    chunk_text=chunk,
                    chunk_index=i,
                    chunk_size=len(chunk),  # chunk_size ì„¤ì •
                    embedding_vector=embedding,
                    tokens=len(chunk.split()) if chunk else 0  # í† í° ìˆ˜ ì„¤ì •
                )
                db.add(embedding_obj)
                if i % 10 == 0:  # 10ê°œë§ˆë‹¤ ë¡œê·¸
                    logger.debug(f"ğŸ“ ì„ë² ë”© ê°ì²´ ì¶”ê°€: {i+1}/{len(chunks)}")
            
            logger.info(f"ğŸ’¾ ì„ë² ë”© ì»¤ë°‹ ì‹œì‘: document_id={document.id}")
            db.commit()
            logger.info(f"âœ… ì„ë² ë”© ìƒì„± ì™„ë£Œ: {document.id} - {len(embeddings)}ê°œ")
            return True
            
        except Exception as e:
            logger.error(f"ğŸ’¥ ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {document.id} - {str(e)}", exc_info=True)
            try:
                db.rollback()
                logger.info(f"ğŸ”„ DB ë¡¤ë°± ì™„ë£Œ: document_id={document.id}")
            except Exception as rollback_error:
                logger.error(f"ğŸ’¥ DB ë¡¤ë°± ì‹¤íŒ¨: {rollback_error}")
            return False
    
    async def reprocess_document(self, document: Document, db: Session) -> bool:
        """
        ë¬¸ì„œ ì¬ì²˜ë¦¬
        
        Args:
            document: ì¬ì²˜ë¦¬í•  ë¬¸ì„œ
            db: ë°ì´í„°ë² ì´ìŠ¤ ì„¸ì…˜
            
        Returns:
            ì¬ì²˜ë¦¬ ì„±ê³µ ì—¬ë¶€
        """
        try:
            logger.info(f"ğŸ”„ ë¬¸ì„œ ì¬ì²˜ë¦¬ ì‹œì‘: document_id={document.id}, filename={document.filename}")
            
            # ê¸°ì¡´ ì„ë² ë”© ê°œìˆ˜ í™•ì¸
            existing_embeddings = db.query(Embedding).filter(Embedding.document_id == document.id).count()
            logger.info(f"ğŸ—‘ï¸ ì‚­ì œí•  ê¸°ì¡´ ì„ë² ë”© ê°œìˆ˜: {existing_embeddings}")
            
            # ê¸°ì¡´ ì„ë² ë”© ì‚­ì œ
            deleted_count = db.query(Embedding).filter(Embedding.document_id == document.id).delete()
            logger.info(f"âœ‚ï¸ ì„ë² ë”© ì‚­ì œ ì™„ë£Œ: {deleted_count}ê°œ")
            
            db.commit()
            logger.info(f"ğŸ’¾ ì„ë² ë”© ì‚­ì œ ì»¤ë°‹ ì™„ë£Œ")
            
            # ë¬¸ì„œ ë‹¤ì‹œ ì²˜ë¦¬
            logger.info(f"ğŸš€ ë¬¸ì„œ ë‹¤ì‹œ ì²˜ë¦¬ ì‹œì‘: document_id={document.id}")
            result = await self.process_document(document, db)
            logger.info(f"âœ… ë¬¸ì„œ ì¬ì²˜ë¦¬ ì™„ë£Œ: document_id={document.id}, success={result}")
            
            return result
            
        except Exception as e:
            logger.error(f"ğŸ’¥ ë¬¸ì„œ ì¬ì²˜ë¦¬ ì¤‘ ì˜ˆì™¸ ë°œìƒ: document_id={document.id}, error={str(e)}", exc_info=True)
            return False

# ì „ì—­ ë¬¸ì„œ ì²˜ë¦¬ ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤
_document_service: Optional[DocumentProcessingService] = None

def get_document_service() -> DocumentProcessingService:
    """ë¬¸ì„œ ì²˜ë¦¬ ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global _document_service
    if _document_service is None:
        _document_service = DocumentProcessingService()
    return _document_service

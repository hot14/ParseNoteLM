"""
OpenAI API í†µí•© ì„œë¹„ìŠ¤
"""
import asyncio
import logging
from typing import List, Dict, Any, Optional, NamedTuple
from openai import AsyncOpenAI
from app.config import settings

logger = logging.getLogger(__name__)

class OpenAIChatResponse(NamedTuple):
    """OpenAI ì±„íŒ… ì‘ë‹µ íƒ€ì…"""
    message: str
    tokens_used: int

class OpenAIService:
    """OpenAI API ì„œë¹„ìŠ¤ í´ë˜ìŠ¤"""
    
    def __init__(self):
        """OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”"""
        if not settings.OPENAI_API_KEY:
            raise ValueError("OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
        
        self.client = AsyncOpenAI(
            api_key=settings.OPENAI_API_KEY
        )
        
        # ëª¨ë¸ ì„¤ì •
        self.chat_model = "gpt-3.5-turbo"
        self.embedding_model = "text-embedding-3-small"
        
        # ì„¤ì •ê°’
        self.max_tokens = 1000
        self.temperature = 0.7
        self.max_retries = 3
        self.timeout = 30.0
    
    async def generate_summary(self, text: str, max_length: int = 200) -> str:
        """
        í…ìŠ¤íŠ¸ ìš”ì•½ ìƒì„±
        
        Args:
            text: ìš”ì•½í•  í…ìŠ¤íŠ¸
            max_length: ìµœëŒ€ ìš”ì•½ ê¸¸ì´
            
        Returns:
            ìƒì„±ëœ ìš”ì•½ í…ìŠ¤íŠ¸
        """
        try:
            # í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ê¸´ ê²½ìš° ì˜ë¼ë‚´ê¸°
            if len(text) > 8000:
                text = text[:8000] + "..."
            
            prompt = f"""
ë‹¤ìŒ í…ìŠ¤íŠ¸ë¥¼ {max_length}ì ì´ë‚´ë¡œ ê°„ê²°í•˜ê³  ëª…í™•í•˜ê²Œ ìš”ì•½í•´ì£¼ì„¸ìš”. 
í•µì‹¬ ë‚´ìš©ê³¼ ì£¼ìš” í¬ì¸íŠ¸ë¥¼ í¬í•¨í•˜ì—¬ í•œêµ­ì–´ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”.

í…ìŠ¤íŠ¸:
{text}

ìš”ì•½:
"""
            
            response = await self.client.chat.completions.create(
                model=self.chat_model,
                messages=[
                    {"role": "user", "content": prompt}
                ],
                max_tokens=self.max_tokens,
                temperature=self.temperature,
                timeout=self.timeout
            )
            
            summary = response.choices[0].message.content.strip()
            logger.info(f"í…ìŠ¤íŠ¸ ìš”ì•½ ìƒì„± ì™„ë£Œ. ì›ë³¸ ê¸¸ì´: {len(text)}, ìš”ì•½ ê¸¸ì´: {len(summary)}")
            
            return summary
            
        except Exception as e:
            logger.error(f"í…ìŠ¤íŠ¸ ìš”ì•½ ìƒì„± ì‹¤íŒ¨: {str(e)}")
            raise Exception(f"ìš”ì•½ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
    
    async def generate_embedding(self, text: str) -> List[float]:
        """
        í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„±
        
        Args:
            text: ì„ë² ë”©ì„ ìƒì„±í•  í…ìŠ¤íŠ¸
            
        Returns:
            ìƒì„±ëœ ì„ë² ë”© ë²¡í„°
        """
        try:
            # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
            text = text.strip()
            if not text:
                raise ValueError("ë¹ˆ í…ìŠ¤íŠ¸ë¡œëŠ” ì„ë² ë”©ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            
            # í…ìŠ¤íŠ¸ ê¸¸ì´ ì œí•œ (8192 í† í° ì œí•œ)
            if len(text) > 6000:
                text = text[:6000]
            
            response = await self.client.embeddings.create(
                model=self.embedding_model,
                input=text,
                timeout=self.timeout
            )
            
            embedding = response.data[0].embedding
            logger.info(f"ì„ë² ë”© ìƒì„± ì™„ë£Œ. í…ìŠ¤íŠ¸ ê¸¸ì´: {len(text)}, ì„ë² ë”© ì°¨ì›: {len(embedding)}")
            
            return embedding
            
        except Exception as e:
            logger.error(f"ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {str(e)}")
            raise Exception(f"ì„ë² ë”© ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
    
    async def generate_embeddings_batch(self, texts: List[str]) -> List[List[float]]:
        """
        í…ìŠ¤íŠ¸ ëª©ë¡ì— ëŒ€í•œ ë°°ì¹˜ ì„ë² ë”© ìƒì„±
        
        Args:
            texts: ì„ë² ë”©ì„ ìƒì„±í•  í…ìŠ¤íŠ¸ ëª©ë¡
            
        Returns:
            ìƒì„±ëœ ì„ë² ë”© ë²¡í„° ëª©ë¡
        """
        try:
            if not texts:
                return []
            
            # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
            processed_texts = []
            for text in texts:
                text = text.strip()
                if text:
                    if len(text) > 6000:
                        text = text[:6000]
                    processed_texts.append(text)
            
            if not processed_texts:
                return []
            
            response = await self.client.embeddings.create(
                model=self.embedding_model,
                input=processed_texts,
                timeout=self.timeout * 2  # ë°°ì¹˜ ì²˜ë¦¬ëŠ” ë” ê¸´ íƒ€ì„ì•„ì›ƒ
            )
            
            embeddings = [data.embedding for data in response.data]
            logger.info(f"ë°°ì¹˜ ì„ë² ë”© ìƒì„± ì™„ë£Œ. í…ìŠ¤íŠ¸ ìˆ˜: {len(processed_texts)}")
            
            return embeddings
            
        except Exception as e:
            logger.error(f"ë°°ì¹˜ ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {str(e)}")
            raise Exception(f"ë°°ì¹˜ ì„ë² ë”© ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
    
    async def analyze_document(self, content: str) -> Dict[str, Any]:
        """
        ë¬¸ì„œ ë¶„ì„ (ìš”ì•½ + í‚¤ì›Œë“œ ì¶”ì¶œ + ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜)
        
        Args:
            content: ë¶„ì„í•  ë¬¸ì„œ ë‚´ìš©
            
        Returns:
            ë¶„ì„ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        try:
            if len(content) > 8000:
                content = content[:8000] + "..."
            
            prompt = f"""
ë‹¤ìŒ ë¬¸ì„œë¥¼ ë¶„ì„í•˜ì—¬ JSON í˜•ì‹ìœ¼ë¡œ ê²°ê³¼ë¥¼ ì œê³µí•´ì£¼ì„¸ìš”:

1. ìš”ì•½ (200ì ì´ë‚´)
2. ì£¼ìš” í‚¤ì›Œë“œ (5ê°œ ì´ë‚´)
3. ë¬¸ì„œ ì¹´í…Œê³ ë¦¬ (í•™ìˆ , ê¸°ìˆ , ì¼ë°˜, ë²•ë¥ , ì˜ë£Œ ì¤‘ í•˜ë‚˜)
4. ì£¼ì œ ë¶„ì•¼
5. ë‚œì´ë„ (ì´ˆê¸‰, ì¤‘ê¸‰, ê³ ê¸‰)

ë¬¸ì„œ ë‚´ìš©:
{content}

ì‘ë‹µ í˜•ì‹:
{{
    "summary": "ë¬¸ì„œ ìš”ì•½",
    "keywords": ["í‚¤ì›Œë“œ1", "í‚¤ì›Œë“œ2", "í‚¤ì›Œë“œ3"],
    "category": "ë¬¸ì„œ ì¹´í…Œê³ ë¦¬",
    "topic": "ì£¼ì œ ë¶„ì•¼",
    "difficulty": "ë‚œì´ë„"
}}
"""
            
            response = await self.client.chat.completions.create(
                model=self.chat_model,
                messages=[
                    {"role": "user", "content": prompt}
                ],
                max_tokens=1500,
                temperature=0.3,  # ë¶„ì„ì€ ë” ì¼ê´€ëœ ê²°ê³¼ë¥¼ ìœ„í•´ ë‚®ì€ temperature
                timeout=self.timeout
            )
            
            result_text = response.choices[0].message.content.strip()
            
            # JSON íŒŒì‹± ì‹œë„
            try:
                import json
                # JSON ë¶€ë¶„ë§Œ ì¶”ì¶œ
                start_idx = result_text.find('{')
                end_idx = result_text.rfind('}') + 1
                if start_idx != -1 and end_idx != -1:
                    json_text = result_text[start_idx:end_idx]
                    result = json.loads(json_text)
                else:
                    raise ValueError("JSON í˜•ì‹ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            except:
                # JSON íŒŒì‹± ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ê°’ ë°˜í™˜
                result = {
                    "summary": result_text[:200] if result_text else "ìš”ì•½ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤",
                    "keywords": [],
                    "category": "ì¼ë°˜",
                    "topic": "ê¸°íƒ€",
                    "difficulty": "ì¤‘ê¸‰"
                }
            
            logger.info(f"ë¬¸ì„œ ë¶„ì„ ì™„ë£Œ. ë‚´ìš© ê¸¸ì´: {len(content)}")
            return result
            
        except Exception as e:
            logger.error(f"ë¬¸ì„œ ë¶„ì„ ì‹¤íŒ¨: {str(e)}")
            raise Exception(f"ë¬¸ì„œ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
    
    async def generate_answer(self, question: str, context: str) -> str:
        """
        RAG ê¸°ë°˜ ì§ˆì˜ì‘ë‹µ
        
        Args:
            question: ì‚¬ìš©ì ì§ˆë¬¸
            context: ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸
            
        Returns:
            ìƒì„±ëœ ë‹µë³€
        """
        try:
            prompt = f"""
ì£¼ì–´ì§„ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì •í™•í•˜ê³  ìœ ìš©í•œ ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”.
ì»¨í…ìŠ¤íŠ¸ì— ì—†ëŠ” ì •ë³´ëŠ” ì¶”ì¸¡í•˜ì§€ ë§ê³ , ì»¨í…ìŠ¤íŠ¸ ë‚´ì—ì„œë§Œ ë‹µë³€í•´ì£¼ì„¸ìš”.
ë‹µë³€ì€ í•œêµ­ì–´ë¡œ ì‘ì„±í•˜ê³ , ëª…í™•í•˜ê³  ì´í•´í•˜ê¸° ì‰½ê²Œ ì„¤ëª…í•´ì£¼ì„¸ìš”.

ì»¨í…ìŠ¤íŠ¸:
{context}

ì§ˆë¬¸: {question}

ë‹µë³€:
"""
            
            response = await self.client.chat.completions.create(
                model=self.chat_model,
                messages=[
                    {"role": "user", "content": prompt}
                ],
                max_tokens=1500,
                temperature=0.7,
                timeout=self.timeout
            )
            
            answer = response.choices[0].message.content.strip()
            logger.info(f"RAG ë‹µë³€ ìƒì„± ì™„ë£Œ. ì§ˆë¬¸: {question[:50]}...")
            
            return answer
            
        except Exception as e:
            logger.error(f"RAG ë‹µë³€ ìƒì„± ì‹¤íŒ¨: {str(e)}")
            raise Exception(f"ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
    
    async def generate_chat_response(self, prompt: str) -> OpenAIChatResponse:
        """
        ì±„íŒ… ì‘ë‹µ ìƒì„± (RAGìš©)
        
        Args:
            prompt: ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸ (ì»¨í…ìŠ¤íŠ¸ í¬í•¨)
            
        Returns:
            ChatResponse ê°ì²´ (message, tokens_used)
        """
        try:
            response = await self.client.chat.completions.create(
                model=self.chat_model,
                messages=[
                    {"role": "user", "content": prompt}
                ],
                max_tokens=1500,
                temperature=0.7,
                timeout=self.timeout
            )
            
            message = response.choices[0].message.content.strip()
            tokens_used = response.usage.total_tokens if response.usage else 0
            
            logger.info(f"ì±„íŒ… ì‘ë‹µ ìƒì„± ì™„ë£Œ. í† í° ì‚¬ìš©ëŸ‰: {tokens_used}")
            
            return OpenAIChatResponse(
                message=message,
                tokens_used=tokens_used
            )
            
        except Exception as e:
            logger.error(f"ì±„íŒ… ì‘ë‹µ ìƒì„± ì‹¤íŒ¨: {str(e)}")
            raise Exception(f"ì±„íŒ… ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
    
    async def generate_chat_completion(self, messages: List[Dict[str, str]], max_tokens: int = 1500, temperature: float = 0.3) -> Dict[str, Any]:
        """
        í‘œì¤€ OpenAI ì±„íŒ… ì™„ì„± API í˜¸ì¶œ
        
        Args:
            messages: ì±„íŒ… ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸ (role, content í¬í•¨)
            max_tokens: ìµœëŒ€ í† í° ìˆ˜
            temperature: ì°½ì˜ì„± ìˆ˜ì¤€ (0.0~1.0)
            
        Returns:
            OpenAI API ì‘ë‹µ ë”•ì…”ë„ˆë¦¬
        """
        try:
            logger.info(f"ğŸ’¬ OpenAI ì±„íŒ… ì™„ì„± ìš”ì²­: {len(messages)}ê°œ ë©”ì‹œì§€, max_tokens={max_tokens}")
            
            response = await self.client.chat.completions.create(
                model=self.chat_model,
                messages=messages,
                max_tokens=max_tokens,
                temperature=temperature,
                timeout=self.timeout
            )
            
            content = response.choices[0].message.content.strip()
            tokens_used = response.usage.total_tokens if response.usage else 0
            
            logger.info(f"âœ… ì±„íŒ… ì™„ì„± ì„±ê³µ: {tokens_used} í† í° ì‚¬ìš©")
            
            return {
                "content": content,
                "usage": {
                    "total_tokens": tokens_used,
                    "prompt_tokens": response.usage.prompt_tokens if response.usage else 0,
                    "completion_tokens": response.usage.completion_tokens if response.usage else 0
                }
            }
            
        except Exception as e:
            logger.error(f"âŒ ì±„íŒ… ì™„ì„± ì‹¤íŒ¨: {str(e)}")
            raise Exception(f"ì±„íŒ… ì™„ì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")

# ì „ì—­ OpenAI ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤
_openai_service: Optional[OpenAIService] = None

def get_openai_service() -> OpenAIService:
    """OpenAI ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global _openai_service
    if _openai_service is None:
        _openai_service = OpenAIService()
    return _openai_service

"""
RAG(ê²€ìƒ‰ ì¦ê°• ìƒì„±) ì‹œìŠ¤í…œ êµ¬í˜„
ë¬¸ì„œ ê²€ìƒ‰, ì²­í‚¹, ë²¡í„°í™”, ìœ ì‚¬ë„ ê²€ìƒ‰ì„ ë‹´ë‹¹í•©ë‹ˆë‹¤.
"""

import os
import logging
from typing import List, Dict, Any, Optional, Tuple
from uuid import UUID

import numpy as np
from sqlalchemy.orm import Session
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.schema import Document

from app.core.database import get_db
from app.models.document import Document as DocumentModel
from app.models.embedding import Embedding
from app.config import settings

logger = logging.getLogger(__name__)


class DocumentChunker:
    """ë¬¸ì„œë¥¼ ì˜ë¯¸ì  ë‹¨ìœ„ë¡œ ë¶„í• í•˜ëŠ” í´ë˜ìŠ¤"""
    
    def __init__(self, chunk_size: int = 512, chunk_overlap: int = 50):
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            length_function=len,
            separators=["\n\n", "\n", " ", ""]
        )
    
    def chunk_document(self, content: str, document_id: str) -> List[Dict[str, Any]]:
        """ë¬¸ì„œë¥¼ ì²­í¬ë¡œ ë¶„í• """
        try:
            chunks = self.text_splitter.split_text(content)
            
            chunk_data = []
            for i, chunk in enumerate(chunks):
                if len(chunk.strip()) > 20:  # ë„ˆë¬´ ì§§ì€ ì²­í¬ ì œì™¸
                    chunk_data.append({
                        "text": chunk.strip(),
                        "document_id": document_id,
                        "chunk_index": i,
                        "metadata": {
                            "length": len(chunk),
                            "position": i,
                            "total_chunks": len(chunks)
                        }
                    })
            
            logger.info(f"ë¬¸ì„œ {document_id}: {len(chunks)}ê°œ ì²­í¬ ìƒì„± â†’ {len(chunk_data)}ê°œ ìœ íš¨ ì²­í¬")
            return chunk_data
            
        except Exception as e:
            logger.error(f"ë¬¸ì„œ ì²­í‚¹ ì‹¤íŒ¨ {document_id}: {e}")
            return []


class VectorRetriever:
    """ë²¡í„° ê¸°ë°˜ ë¬¸ì„œ ê²€ìƒ‰ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.embeddings = OpenAIEmbeddings(
            openai_api_key=settings.OPENAI_API_KEY,
            model="text-embedding-ada-002"
        )
        self.vector_stores: Dict[str, FAISS] = {}  # í”„ë¡œì íŠ¸ë³„ ë²¡í„° ìŠ¤í† ì–´
    
    async def create_embeddings(self, texts: List[str]) -> List[List[float]]:
        """í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸ë¥¼ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜"""
        try:
            # OpenAI API ìš°ì„  ì‚¬ìš©
            embeddings = await self.embeddings.aembed_documents(texts)
            return embeddings
        except Exception as e:
            logger.error(f"OpenAI ì„ë² ë”© ì‹¤íŒ¨: {e}")
            return []
    
    async def add_documents_to_store(
        self, 
        project_id: str, 
        chunk_data: List[Dict[str, Any]]
    ) -> bool:
        """ë¬¸ì„œ ì²­í¬ë“¤ì„ ë²¡í„° ìŠ¤í† ì–´ì— ì¶”ê°€"""
        try:
            if not chunk_data:
                return False
            
            # í…ìŠ¤íŠ¸ ì¶”ì¶œ
            texts = [chunk["text"] for chunk in chunk_data]
            
            # ì„ë² ë”© ìƒì„±
            embeddings = await self.create_embeddings(texts)
            
            # LangChain Document ê°ì²´ ìƒì„±
            documents = []
            for chunk, embedding in zip(chunk_data, embeddings):
                doc = Document(
                    page_content=chunk["text"],
                    metadata={
                        "document_id": chunk["document_id"],
                        "chunk_index": chunk["chunk_index"],
                        **chunk["metadata"]
                    }
                )
                documents.append(doc)
            
            # FAISS ë²¡í„° ìŠ¤í† ì–´ ìƒì„±/ì—…ë°ì´íŠ¸
            if project_id in self.vector_stores:
                # ê¸°ì¡´ ìŠ¤í† ì–´ì— ì¶”ê°€
                self.vector_stores[project_id].add_documents(documents)
            else:
                # ìƒˆ ìŠ¤í† ì–´ ìƒì„±
                self.vector_stores[project_id] = FAISS.from_documents(
                    documents, 
                    self.embeddings
                )
            
            logger.info(f"í”„ë¡œì íŠ¸ {project_id}: {len(documents)}ê°œ ë¬¸ì„œ ì²­í¬ ë²¡í„°í™” ì™„ë£Œ")
            return True
            
        except Exception as e:
            logger.error(f"ë²¡í„° ìŠ¤í† ì–´ ì¶”ê°€ ì‹¤íŒ¨: {e}")
            return False
    
    async def search_similar_documents(
        self, 
        project_id: str, 
        query: str, 
        k: int = 5,
        score_threshold: float = 0.5
    ) -> List[Dict[str, Any]]:
        """ìœ ì‚¬í•œ ë¬¸ì„œ ì²­í¬ ê²€ìƒ‰"""
        try:
            if project_id not in self.vector_stores:
                logger.warning(f"í”„ë¡œì íŠ¸ {project_id}ì˜ ë²¡í„° ìŠ¤í† ì–´ê°€ ì—†ìŠµë‹ˆë‹¤")
                return []
            
            vector_store = self.vector_stores[project_id]
            
            # ìœ ì‚¬ë„ ê²€ìƒ‰ ìˆ˜í–‰
            docs_with_scores = vector_store.similarity_search_with_score(
                query, k=k
            )
            
            # ì ìˆ˜ í•„í„°ë§ ë° ê²°ê³¼ í¬ë§·íŒ…
            results = []
            for doc, score in docs_with_scores:
                # FAISSëŠ” ê±°ë¦¬ë¥¼ ë°˜í™˜í•˜ë¯€ë¡œ ìœ ì‚¬ë„ë¡œ ë³€í™˜
                similarity = 1.0 / (1.0 + score)
                
                if similarity >= score_threshold:
                    results.append({
                        "content": doc.page_content,
                        "similarity": similarity,
                        "metadata": doc.metadata,
                        "document_id": doc.metadata.get("document_id"),
                        "chunk_index": doc.metadata.get("chunk_index")
                    })
            
            # ìœ ì‚¬ë„ ê¸°ì¤€ ì •ë ¬
            results.sort(key=lambda x: x["similarity"], reverse=True)
            
            logger.info(f"ì¿¼ë¦¬ '{query}': {len(results)}ê°œ ìœ ì‚¬ ë¬¸ì„œ ë°œê²¬")
            return results
            
        except Exception as e:
            logger.error(f"ë¬¸ì„œ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []
    
    def save_vector_store(self, project_id: str, path: str) -> bool:
        """ë²¡í„° ìŠ¤í† ì–´ë¥¼ íŒŒì¼ë¡œ ì €ì¥"""
        try:
            if project_id in self.vector_stores:
                os.makedirs(os.path.dirname(path), exist_ok=True)
                self.vector_stores[project_id].save_local(path)
                logger.info(f"ë²¡í„° ìŠ¤í† ì–´ ì €ì¥ ì™„ë£Œ: {path}")
                return True
            return False
        except Exception as e:
            logger.error(f"ë²¡í„° ìŠ¤í† ì–´ ì €ì¥ ì‹¤íŒ¨: {e}")
            return False
    
    def load_vector_store(self, project_id: str, path: str) -> bool:
        """íŒŒì¼ì—ì„œ ë²¡í„° ìŠ¤í† ì–´ ë¡œë“œ"""
        try:
            if os.path.exists(path):
                # FAISS ë²„ì „ í˜¸í™˜ì„±ì„ ìœ„í•œ ë‹¤ì¤‘ ì‹œë„ ë°©ì‹
                try:
                    # ìµœì‹  FAISS ë²„ì „ì—ì„œ ë¨¼ì € ì‹œë„
                    self.vector_stores[project_id] = FAISS.load_local(
                        path, 
                        self.embeddings,
                        allow_dangerous_deserialization=True
                    )
                except TypeError:
                    # êµ¬ ë²„ì „ FAISSì—ì„œëŠ” allow_dangerous_deserialization íŒŒë¼ë¯¸í„° ì—†ì´ ì‹œë„
                    logger.info("FAISS êµ¬ ë²„ì „ í˜¸í™˜ ëª¨ë“œë¡œ ë²¡í„° ìŠ¤í† ì–´ ë¡œë“œë¥¼ ì‹œë„í•©ë‹ˆë‹¤.")
                    self.vector_stores[project_id] = FAISS.load_local(
                        path, 
                        self.embeddings
                    )
                
                logger.info(f"ë²¡í„° ìŠ¤í† ì–´ ë¡œë“œ ì™„ë£Œ: {path}")
                return True
            return False
        except Exception as e:
            logger.error(f"ë²¡í„° ìŠ¤í† ì–´ ë¡œë“œ ì‹¤íŒ¨: {e}")
            logger.error(f"  ğŸ“ íŒŒì¼: {__file__}:{self.load_vector_store.__code__.co_firstlineno}")
            logger.error(f"  ğŸ”§ í•¨ìˆ˜: {self.load_vector_store.__name__}")
            return False


class RAGService:
    """RAG ì‹œìŠ¤í…œì˜ ë©”ì¸ ì„œë¹„ìŠ¤ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.chunker = DocumentChunker()
        self.retriever = VectorRetriever()
        from app.core.config import settings
        self.vector_store_base_path = settings.get_absolute_vector_store_dir
    
    async def process_document_for_rag(
        self, 
        document_id: int, 
        db: Session
    ) -> bool:
        """ë¬¸ì„œë¥¼ RAG ì‹œìŠ¤í…œì— ì¶”ê°€ ì²˜ë¦¬"""
        try:
            # ë¬¸ì„œ ì¡°íšŒ
            document = db.query(DocumentModel).filter(
                DocumentModel.id == document_id
            ).first()
            
            if not document or not document.content:
                logger.warning(f"ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ê±°ë‚˜ ë‚´ìš©ì´ ì—†ìŒ: {document_id}")
                return False
            
            # ë¬¸ì„œ ì²­í‚¹
            chunks = self.chunker.chunk_document(
                document.content, 
                str(document_id)
            )
            
            if not chunks:
                logger.warning(f"ë¬¸ì„œ ì²­í‚¹ ì‹¤íŒ¨: {document_id}")
                return False
            
            # ë²¡í„° ìŠ¤í† ì–´ì— ì¶”ê°€
            success = await self.retriever.add_documents_to_store(
                str(document.project_id), 
                chunks
            )
            
            if success:
                # ë²¡í„° ìŠ¤í† ì–´ ì €ì¥
                store_path = os.path.join(
                    self.vector_store_base_path, 
                    str(document.project_id)
                )
                self.retriever.save_vector_store(
                    str(document.project_id), 
                    store_path
                )
                
                # ì„ë² ë”© ë©”íƒ€ë°ì´í„° DBì— ì €ì¥
                for chunk in chunks:
                    # ì²­í¬ì— ëŒ€í•œ ì„ë² ë”© ë²¡í„° ìƒì„±
                    chunk_embedding = await self.retriever.embeddings.aembed_query(chunk["text"])
                    
                    # ë””ë²„ê¹…ì„ ìœ„í•œ ë¡œê·¸ ì¶”ê°€
                    chunk_text = chunk["text"]
                    chunk_size_val = len(chunk_text)
                    logger.info(f"ì²­í¬ ì²˜ë¦¬ ì¤‘: chunk_size={chunk_size_val}, text_length={len(chunk_text)}")
                    
                    embedding = Embedding(
                        document_id=document_id,
                        chunk_text=chunk_text,
                        chunk_size=chunk_size_val,  # ì²­í¬ í¬ê¸° (ë¬¸ì ìˆ˜) ì¶”ê°€
                        chunk_index=chunk["chunk_index"],
                        embedding_vector=chunk_embedding,  # ì„ë² ë”© ë²¡í„° ì¶”ê°€
                        embedding_model="text-embedding-ada-002",
                        vector_dimension=len(chunk_embedding) if chunk_embedding else 1536,
                        tokens=len(chunk_text.split()),  # ëŒ€ëµì ì¸ í† í° ìˆ˜
                        document_metadata=chunk["metadata"]  # metadata_ â†’ document_metadataë¡œ ìˆ˜ì •
                    )
                    db.add(embedding)
                
                db.commit()
                
                # ë¬¸ì„œì˜ chunk_count ì—…ë°ì´íŠ¸
                document.chunk_count = len(chunks)
                db.commit()
                
                logger.info(f"ë¬¸ì„œ RAG ì²˜ë¦¬ ì™„ë£Œ: {document_id}, chunks: {len(chunks)}")
                return True
            
            return False
            
        except Exception as e:
            logger.error(f"RAG ë¬¸ì„œ ì²˜ë¦¬ ì‹¤íŒ¨ {document_id}: {e}")
            db.rollback()
            return False
    
    async def load_project_vector_store(self, project_id: str) -> bool:
        """í”„ë¡œì íŠ¸ì˜ ë²¡í„° ìŠ¤í† ì–´ ë¡œë“œ"""
        store_path = os.path.join(
            self.vector_store_base_path, 
            project_id
        )
        return self.retriever.load_vector_store(project_id, store_path)
    
    async def search_documents(
        self, 
        project_id: str, 
        query: str, 
        max_results: int = 5,
        score_threshold: float = 0.3  # ì„ê³„ê°’ì„ ë‚®ì¶¤
    ) -> List[Dict[str, Any]]:
        """í”„ë¡œì íŠ¸ ë‚´ ë¬¸ì„œ ê²€ìƒ‰"""
        # ë²¡í„° ìŠ¤í† ì–´ê°€ ë©”ëª¨ë¦¬ì— ì—†ìœ¼ë©´ ë¡œë“œ
        if str(project_id) not in self.retriever.vector_stores:
            await self.load_project_vector_store(str(project_id))
        
        # ì¿¼ë¦¬ í™•ì¥ - í•œêµ­ì–´/ì˜ì–´ ë™ì˜ì–´ ì¶”ê°€
        expanded_queries = [query]
        
        # ì˜¨í†¨ë¡œì§€ ê´€ë ¨ ë™ì˜ì–´ ì¶”ê°€
        query_lower = query.lower()
        if "ì˜¨í†¨ë¡œì§€" in query_lower:
            expanded_queries.extend(["ontology", "ì¡´ì¬ë¡ ", "ê°œë…ì²´ê³„", "ì§€ì‹ì²´ê³„", "ê°œë…ëª¨ë¸", "ì§€ì‹ëª¨ë¸"])
        elif "ontology" in query_lower:
            expanded_queries.extend(["ì˜¨í†¨ë¡œì§€", "ì¡´ì¬ë¡ ", "ê°œë…ì²´ê³„", "ì§€ì‹ì²´ê³„", "ê°œë…ëª¨ë¸", "ì§€ì‹ëª¨ë¸"])
        elif "ì¡´ì¬ë¡ " in query_lower:
            expanded_queries.extend(["ì˜¨í†¨ë¡œì§€", "ontology", "ê°œë…ì²´ê³„", "ì§€ì‹ì²´ê³„"])
        elif any(keyword in query_lower for keyword in ["ê°œë…ì²´ê³„", "ì§€ì‹ì²´ê³„", "ê°œë…ëª¨ë¸", "ì§€ì‹ëª¨ë¸"]):
            expanded_queries.extend(["ì˜¨í†¨ë¡œì§€", "ontology", "ì¡´ì¬ë¡ "])
        
        logger.info(f"ì¿¼ë¦¬ í™•ì¥: '{query}' â†’ {expanded_queries}")
        
        # ëª¨ë“  í™•ì¥ëœ ì¿¼ë¦¬ë¡œ ê²€ìƒ‰ ìˆ˜í–‰
        all_results = []
        for expanded_query in expanded_queries:
            results = await self.retriever.search_similar_documents(
                str(project_id), 
                expanded_query, 
                k=max_results,
                score_threshold=score_threshold
            )
            all_results.extend(results)
        
        # ì¤‘ë³µ ì œê±° ë° ìœ ì‚¬ë„ ê¸°ì¤€ ì •ë ¬
        unique_results = {}
        for result in all_results:
            key = f"{result['document_id']}_{result['chunk_index']}"
            if key not in unique_results or result['similarity'] > unique_results[key]['similarity']:
                unique_results[key] = result
        
        sorted_results = sorted(unique_results.values(), key=lambda x: x['similarity'], reverse=True)
        return sorted_results[:max_results]


# ì „ì—­ RAG ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤
rag_service = RAGService()


async def get_rag_service() -> RAGService:
    """RAG ì„œë¹„ìŠ¤ ì˜ì¡´ì„± ì£¼ì…"""
    return rag_service
